seed: 1
deterministic: true
policy:
  seed: __AUTO__
  deterministic: __AUTO__
  params:
    algo:
      name: a2c_continuous_controller

    model:
      name: continuous_a2c_logstd_controller

    network:
      name: actor_critic_controller
      controller: __AUTO__
      separate: True
      space:
        continuous:
          use_tanh: True
          mu_activation: None
          sigma_activation: None
          mu_init:
            name: orthogonal
            gain: 0.01
          sigma_init:
            name: const_initializer
            val: 0

          fixed_sigma: True
      mlp:
        units: [256, 128, 64]
        activation: elu

        initializer:
          # pytorch
          name: orthogonal
          gain: 1.41

        regularizer:
          name: 'None' #'l2_regularizer'

    load_checkpoint: False
    load_path: path
    load_optimizer_state: False

    config:
      freeze: False
      deterministic_when_frozen: True
      max_epochs: 5000
      reward_shaper:
        scale_value: 1.0
      normalize_advantage: True
      gamma: 0.99
      tau: 0.95
      learning_rate: 3e-4
      name: __AUTO__
      score_to_win: 10000
      grad_norm: 1.0
      entropy_coef: 0.0
      truncate_grads: True
      env_name: __AUTO__                    # This will automatically be filled from env config below
      env_config: __AUTO__                  # This will automatically be filled with env config from below
      ppo: True
      e_clip: 0.2
      num_actors: __AUTO__
      steps_num: 32
      minibatch_size: 16384
      mini_epochs: 4
      critic_coef: 2.0
      clip_value: False
      lr_schedule: adaptive
      lr_threshold: 0.016
      normalize_input: True
      normalize_value: True
      seq_length: 8
      bounds_loss_coef: 0.0001
      mixed_precision: False
      save_frequency: 50
      logdir: __AUTO__
      player:
        games_num: 10
